### ‚úÖ Pros:
- Human-like, natural reasoning
- Flexible and expressive
- Learns when to sketch, code, or explain
- Easily extendable via system prompt design

### ‚ö†Ô∏è Limitations:
- Slightly less deterministic (non-reproducible outputs without seed control)
- Requires OpenAI API access
- Sketches must still be validated or filtered for coherence

---

## üîÑ Our Extension: Multi-Domain LLM-Guided Dataset Generation

In this project, we build on Visual Sketchpad's approach by:

- Expanding to **new scientific and visual domains**
- Creating a **modular dataset generator** for scalable generation
- Automatically parsing GPT outputs for `<sketch>` instructions
- **Rendering sketches using our own tools**
- Building datasets for training **R1-style models** with unified multimodal CoT

---

## **Project Structure**

| Folder/File            | Purpose                                                               |
|------------------------|-----------------------------------------------------------------------|
| `agent/`               | LLM agent code + OpenAI wrappers + system prompt handling             |
| `tasks/`               | Problem/task definitions and inputs for each domain                   |
| `outputs/`             | Reasoning traces with interleaved sketch and text (generated by GPT)  |
| `generate_datasets/`   | Scripts for creating new problem types (e.g., physics, chemistry)     |
| `interactive_tools/`   | Jupyter notebooks or tools to inspect dataset generation              |
| `vision_experts.md`    | Instructions for running vision expert microservices (SAM, DINO, etc) |

---

## **New Task Domains**

We extend the dataset beyond Visual Sketchpad's original scope to cover new reasoning types:

- **Physics**: Free-body diagrams, projectile motion, vector fields
- **Chemistry**: Reaction pathways, molecule diagrams, equation balancing
- **Calculus**: Derivatives, integrals, critical points, function sketching
- **Turing Machines**: Finite automata, tape transitions
- **Circuit Diagrams**: Voltage division, current loops, resistor networks
- **Maze Problems**: Pathfinding and step-by-step maze solving
- **Tetris**: Predict block fits, optimal rotation, sketch landing zones
- **Scientific Diagrams**: CoT generation from arXiv/Wikipedia figures
- **OCR Caption Reasoning**: Use figure captions to solve visual reasoning problems
- **Code Diagrams**: Flowchart understanding, tracing execution

---

## **Dataset Generation Pipeline**

### Step-by-Step:

1. **Problem Prompt Creation**
   - Use custom scripts (`generate_datasets/`) to create prompts (math, physics, etc.)
   - Scrape external data (e.g., arXiv figures, Wikipedia diagrams) as input images

2. **Trace Generation with GPT-4**
   - Send system + user prompt to OpenAI API
   - GPT-4 generates:
     - Natural language reasoning
     - Optional Python code blocks for sketching
     - Inline visual instructions via `<sketch>` or markdown-style prompts

3. **Code Execution & Sketch Rendering**
   - Sketching code from GPT-4 is executed using local tools (`matplotlib`, `PIL`, `networkx`)
   - Images are saved and numbered: `sketch_0.png`, `sketch_1.png`, etc.

4. **Output Formatting**
   - All output saved in `trace.json`, including:
     - Dialogue history
     - Code blocks
     - Image links
   - Folder structured under `tasks/{domain}/{task_id}/`

5. **Vision Expert Augmentation (Optional)**
   - Call segmentation or detection tools (SAM, DINO, DepthAnything) when needed
   - Used mostly for vision-based tasks or image manipulation

---

## **Vision Expert Integration**

| Tool              | Purpose                                  |
|-------------------|------------------------------------------|
| **SAM**           | Segment Anything Model for region labeling |
| **GroundingDINO** | Object grounding via natural language     |
| **DepthAnything** | 2.5D depth estimation for spatial reasoning |

These tools run as microservices (Flask/Gradio) and are accessed via HTTP API inside your generation pipeline.

### Setup:
- See `vision_experts.md` for setup instructions
- Launch servers on different ports (e.g., 8080‚Äì8082)
- Use `tools.py` to make API calls for:
  - `segment_and_mark()`
  - `detection()`
  - `overlay_images()`
