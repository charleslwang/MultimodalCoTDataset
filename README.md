# **Multimodal Chain-of-Thought (CoT) Dataset Creation Pipeline**

## **Overview**
This project extends the **Visual Sketchpad** framework by creating an **automated pipeline** for generating a **multimodal Chain-of-Thought (CoT)** dataset. The dataset is aimed at training models to perform **multimodal reasoning** by combining **text and images through visual sketching steps**, across multiple domains. This includes traditional tasks such as geometry and graph theory, as well as new domains like physics, chemistry, and interactive tasks like games.

The original Visual Sketchpad paper (NeurIPS 2024) introduced a framework for generating **step-by-step multimodal reasoning traces** that include both text and visual sketches. This project uses and extends their **LLM-powered agent framework**, integrates their **vision experts**, and expands their task coverage to create a broader and more scalable multimodal dataset ‚Äî suitable for training **R1-style any-to-any** multimodal models.

---

## **Visual Sketchpad Recap**

The Visual Sketchpad dataset contains **2046 tasks** across multiple categories. Each task simulates how a multimodal agent might reason step by step, using sketches to support intermediate steps.

### **Domains Covered**:
- **Geometry**: Angle construction, area computation, triangle proofs.
- **Graph Theory**: Node connectivity, isomorphism, pathfinding.
- **Mathematics**: Convexity, parity, functional plots.
- **Games**: Sokoban and similar grid-based reasoning.
- **Visual Reasoning**: Depth maps, segmentation, jigsaw problems.

---

## **How Visual Sketchpad Generates Reasoning Traces**

Unlike traditional rule-based systems, Visual Sketchpad uses **LLM-based agents** (e.g., GPT-4 via OpenAI API) to generate natural language reasoning.

### üîß GPT-Driven Agent Reasoning:
- An **assistant agent** sends prompts to GPT-4 using a notebook-style system message.
- GPT-4 produces:
  - **Natural language explanations**
  - **Python code** to generate sketches or analyze diagrams
- The code is **executed locally** by the system (e.g., `matplotlib`, `PIL`), and visual outputs are saved as part of the reasoning trace.
- The **entire conversation**, including user messages, GPT-4 responses, generated images, and code outputs, is saved as the final trace.

### üß™ Example Trace Format:
```
USER: <img src="triangle.png"> What is angle ABC?

A:
Step 1: Let's draw an auxiliary line parallel to side AC.
```python
# Python code block from GPT-4
draw_auxiliary_line(...)
```

[Sketch: line_added.png]

Step 2: Using the new triangle, we deduce angle ABC is 45¬∞.
```

### ‚úÖ Pros:
- Human-like, natural reasoning
- Flexible and expressive
- Learns when to sketch, code, or explain
- Easily extendable via system prompt design

### ‚ö†Ô∏è Limitations:
- Slightly less deterministic (non-reproducible outputs without seed control)
- Requires OpenAI API access
- Sketches must still be validated or filtered for coherence

---

## üîÑ Our Extension: Multi-Domain LLM-Guided Dataset Generation

In this project, we build on Visual Sketchpad's approach by:

- Expanding to **new scientific and visual domains**
- Creating a **modular dataset generator** for scalable generation
- Automatically parsing GPT outputs for `<sketch>` instructions
- **Rendering sketches using our own tools**
- Building datasets for training **R1-style models** with unified multimodal CoT

---

## **Project Structure**

| Folder/File            | Purpose                                                               |
|------------------------|-----------------------------------------------------------------------|
| `agent/`               | LLM agent code + OpenAI wrappers + system prompt handling             |
| `tasks/`               | Problem/task definitions and inputs for each domain                   |
| `outputs/`             | Reasoning traces with interleaved sketch and text (generated by GPT)  |
| `generate_datasets/`   | Scripts for creating new problem types (e.g., physics, chemistry)     |
| `interactive_tools/`   | Jupyter notebooks or tools to inspect dataset generation              |
| `vision_experts.md`    | Instructions for running vision expert microservices (SAM, DINO, etc) |

---

## **New Task Domains**

We extend the dataset beyond Visual Sketchpad's original scope to cover new reasoning types:

- **Physics**: Free-body diagrams, projectile motion, vector fields
- **Chemistry**: Reaction pathways, molecule diagrams, equation balancing
- **Calculus**: Derivatives, integrals, critical points, function sketching
- **Turing Machines**: Finite automata, tape transitions
- **Circuit Diagrams**: Voltage division, current loops, resistor networks
- **Maze Problems**: Pathfinding and step-by-step maze solving
- **Tetris**: Predict block fits, optimal rotation, sketch landing zones
- **Scientific Diagrams**: CoT generation from arXiv/Wikipedia figures
- **OCR Caption Reasoning**: Use figure captions to solve visual reasoning problems
- **Code Diagrams**: Flowchart understanding, tracing execution

---

## **Dataset Generation Pipeline**

### Step-by-Step:

1. **Problem Prompt Creation**
   - Use custom scripts (`generate_datasets/`) to create prompts (math, physics, etc.)
   - Scrape external data (e.g., arXiv figures, Wikipedia diagrams) as input images

2. **Trace Generation with GPT-4**
   - Send system + user prompt to OpenAI API
   - GPT-4 generates:
     - Natural language reasoning
     - Optional Python code blocks for sketching
     - Inline visual instructions via `<sketch>` or markdown-style prompts

3. **Code Execution & Sketch Rendering**
   - Sketching code from GPT-4 is executed using local tools (`matplotlib`, `PIL`, `networkx`)
   - Images are saved and numbered: `sketch_0.png`, `sketch_1.png`, etc.

4. **Output Formatting**
   - All output saved in `trace.json`, including:
     - Dialogue history
     - Code blocks
     - Image links
   - Folder structured under `tasks/{domain}/{task_id}/`

5. **Vision Expert Augmentation (Optional)**
   - Call segmentation or detection tools (SAM, DINO, DepthAnything) when needed
   - Used mostly for vision-based tasks or image manipulation

---

## **Vision Expert Integration**

| Tool              | Purpose                                  |
|-------------------|------------------------------------------|
| **SAM**           | Segment Anything Model for region labeling |
| **GroundingDINO** | Object grounding via natural language     |
| **DepthAnything** | 2.5D depth estimation for spatial reasoning |

These tools run as microservices (Flask/Gradio) and are accessed via HTTP API inside your generation pipeline.

### Setup:
- See `vision_experts.md` for setup instructions
- Launch servers on different ports (e.g., 8080‚Äì8082)
- Use `tools.py` to make API calls for:
  - `segment_and_mark()`
  - `detection()`
  - `overlay_images()`

---

