# **Multimodal Chain-of-Thought (CoT) Dataset Creation Pipeline**

## **Overview**
This project extends the **Visual Sketchpad** framework by creating an **automated pipeline** for generating a **multimodal Chain-of-Thought (CoT)** dataset. The dataset is aimed at training models to perform **multimodal reasoning** by combining **text and images through visual sketching steps**, across multiple domains. This includes traditional tasks such as geometry and graph theory, as well as new domains like physics, chemistry, and interactive tasks like games.

The original Visual Sketchpad paper (NeurIPS 2024) proposed generating **step-by-step multimodal reasoning traces** for tasks involving sketching as part of the solution. This project uses their **agent framework**, integrates their **vision experts**, and extends their **task domains** to build a much larger and more diverse dataset, ultimately to enable training **R1-style** any-to-any multimodal models.

---

## **Visual Sketchpad Recap**

The Visual Sketchpad paper provides a dataset of **2046 tasks** across several categories. These tasks focus on generating **step-by-step reasoning traces** that combine **textual and visual sketching actions**. Each task simulates how a multimodal agent would reason through a problem, drawing intermediate diagrams along the way.

### **Domains Covered**:
- **Geometry**: Determining angles, constructing lines, and calculating areas.
- **Graph Theory**: Checking connectivity, isomorphism, max flow, etc.
- **Mathematics**: Function analysis (convexity, parity), plotting, etc.
- **Puzzle Games**: Sokoban-style grid puzzles.
- **Vision Tasks**: Spatial reasoning, depth perception, jigsaw assembly.

### **What's in a Task?**
Each task includes:
- A **prompt**: The question the agent must answer.
- A **diagram/image**: Often representing a structure or state to reason about.
- A **reasoning trace**: A sequence of **interleaved natural language + visual sketching steps** (i.e., a multimodal Chain-of-Thought).

---

## **How Visual Sketchpad Generates Reasoning Traces**

Unlike LLM-based approaches, Visual Sketchpad does **not** use GPT-style models to generate reasoning. Instead, it uses a **hand-coded agent system**:

### üîß Agent-Driven Trace Generation:
- A scripted **assistant agent** reasons through the task.
- At each step, it:
  - Outputs a **natural language explanation**
  - Optionally executes **Python code** to render a sketch (e.g., draw a line or plot)
  - Calls **vision tools** (like SAM or GroundingDINO) when needed
- Each step is logged as part of a growing **trace** ‚Äî a combination of text and images.

### üß™ Example Trace Format:
```
USER: <img src="triangle.png"> What is angle ABC?

A:
Step 1: Extend line AB to better isolate angle ABC.
[Sketch: extended_line.png]

Step 2: Now we see that triangle ABC has angle of 45 degrees.
[Answer: 45¬∞]
```

### ‚úÖ Pros:
- Fully controllable and deterministic
- Integrates vision tools cleanly
- Easy to evaluate the impact of sketching

### ‚ùå Cons:
- Not scalable ‚Äî each domain must be hand-coded
- Lacks natural diversity and expressiveness
- Doesn't reflect how learned models will reason in practice

---

## üîÑ Our New Approach: LLM-Generated Traces

In this project, we extend Visual Sketchpad's framework by allowing reasoning traces to be:
- **Generated using OpenAI's API (e.g., GPT-4)** or other LLMs
- **Formatted using <sketch> tags** to describe when visual sketches are needed
- **Paired with scripts that render the described sketches automatically**

This allows for:
- More fluent and diverse reasoning styles
- Scalable dataset creation (10k‚Äì50k+ examples)
- Training models in an **R1-style** framework where sketching becomes a learned behavior

---

## **Project Structure**

This repository consists of several components:

| Folder/File            | Purpose                                                               |
|------------------------|-----------------------------------------------------------------------|
| `agent/`               | Agent framework and utilities for multimodal reasoning                |
| `tasks/`               | Stores problem/task definitions (prompt, image, ground truth, etc.)   |
| `outputs/`             | Stores reasoning traces generated by the agent or LLMs                |
| `generate_datasets/`   | Scripts for automatically generating new domain-specific tasks        |
| `interactive_tools/`   | Utilities for visualizing and validating data                         |
| `vision_experts.md`    | Configuration and server code for tools like SAM, DINO, DepthAnything |

---

## **New Tasks to Be Generated**

We go beyond Visual Sketchpad's original categories with new domains that include both scientific reasoning and complex visual problem-solving:

- **Physics**: Free-body diagrams, motion trajectories, vector fields
- **Chemistry**: Reaction diagrams, molecule structures, balancing equations
- **Calculus**: Derivatives, integrals, tangent lines, optimization plots
- **Turing Machines**: Tape transitions, finite automata
- **Circuit Diagrams**: Resistance, current flow, and circuit analysis tasks
- **Maze Reasoning**: Puzzle-solving, shortest paths
- **Tetris Reasoning**: Predict fit, sketch drop zones
- **Scientific Figures**: Scraped from arXiv/Wikipedia + LLM-generated CoT
- **OCR Captioning**: Reasoning from scientific figure + caption images
- **Code Diagrams**: Flowcharts or code structure ‚Üí step-by-step explanation

---

## **Dataset Generation Pipeline**

### Step-by-Step:

1. **Problem Generation**
   - Use Python scripts to generate prompts (math/physics/graph theory)
   - Optionally scrape diagrams from external sources (e.g. arXiv figures)

2. **Trace Generation**
   - Option 1: Use scripted VisualSketchpad agent for interpretable baselines
   - Option 2: Use OpenAI/GPT-style LLM to generate CoT traces with `<sketch>` blocks

3. **Sketch Rendering**
   - Use `tools.py` (e.g. `AnnotatedImage`) to render described sketches
   - Generate labeled diagrams, plots, visual proofs, or physics diagrams

4. **Formatting**
   - Save `request.json`, `image.png`, `trace.json`, and any intermediate sketches
   - Store under `tasks/{domain}/{id}/`

5. **Optional Agent Evaluation**
   - Run `run_task.py` to evaluate reasoning agents on your dataset
   - Output traces go into `outputs/{domain}/{id}/`

---

## **Reusing Visual Sketchpad's Tasks**

You can reuse and modify the original 2046 tasks from `tasks/seed_tasks/`. These serve as:
- Benchmarks for your generation pipeline
- Templates to ensure task format consistency
- Gold traces for comparing rule-based vs. LLM-based reasoning

---

## **Vision Expert Integration**

Visual Sketchpad integrates vision models as callable microservices:
- **SAM** (Segment Anything Model) for object segmentation
- **GroundingDINO** for object grounding and bounding box detection
- **DepthAnything** for estimating spatial relationships

You can call them directly from `tools.py` during reasoning trace generation or sketch rendering.

### Setup:
1. Launch the servers via `vision_experts.md`
2. Use the API endpoints from `tools.py` to call:
   - `segment_and_mark()`
   - `detection()`
   - `overlay_images()` etc.

---
